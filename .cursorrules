# RBC Backend Assessment - Cursor AI Rules
# Project: Project Tracker API with MCP Integration
# Assessment Duration: 3-5 hours

## Core Development Principles
- Follow clean code principles: DRY, KISS, SOLID
- Use TypeScript strict mode with proper type definitions
- Implement layered architecture: Controllers → Services → Models
- Write small, focused functions (under 20 lines when possible)
- Use meaningful, descriptive naming conventions

## Technology Stack Requirements
- Node.js 18+ with TypeScript
- Express.js framework
- PostgreSQL with Prisma ORM
- Zod for validation
- Jest for testing
- Swagger/OpenAPI for documentation
- MCP SDK for agent integration

## Code Style Guidelines
- Use async/await over promises
- Implement proper error handling with custom error classes
- Use dependency injection for better testability
- Follow REST API conventions
- Add JSDoc comments for public functions
- Use environment variables for configuration

## File Structure Patterns
- Controllers: Handle HTTP requests/responses only
- Services: Contain business logic
- Models: Define data structures and validation
- Middleware: Cross-cutting concerns (auth, validation, logging)
- Utils: Pure helper functions

## Database Conventions
- Use Prisma schema definitions
- Follow snake_case for database columns
- Use camelCase for TypeScript interfaces
- Implement proper relationships and constraints
- Add created_at/updated_at timestamps

## API Design Standards
- Use RESTful endpoints
- Implement proper HTTP status codes
- Add request/response validation
- Include error handling middleware
- Support query parameters for filtering

## MCP Integration Focus
- Create simple, focused MCP tools
- Use structured input/output schemas
- Implement proper error handling in tools
- Add example usage documentation
- Keep agent responses JSON-structured

## Testing Requirements
- Write unit tests for services
- Add integration tests for API endpoints
- Test MCP tool functionality
- Aim for >80% coverage on core features
- Use descriptive test names

## Commit Message Format
- Use conventional commits: feat/fix/docs/test
- Include clear, descriptive messages
- Reference the phase and task number
- Keep commits atomic and focused

## Performance Considerations
- Implement caching for frequent queries
- Use database indexes appropriately
- Add connection pooling
- Implement request rate limiting
- Use pagination for large datasets

## Security Best Practices
- Validate all inputs with Zod
- Sanitize database queries (Prisma handles this)
- Add proper error messages without sensitive data
- Use environment variables for secrets
- Implement request logging

## Assessment Specific Notes
- Focus on core functionality over advanced features
- Demonstrate clean architecture understanding
- Show MCP integration capabilities
- Keep solutions simple but professional
- Document key decisions and trade-offs

## Current Development Phase
Starting Phase 1: Foundation Setup
Next: Initialize TypeScript Express project with proper configuration

## Priority Order
Part 1:
1. Core CRUD operations
2. Database integration
3. Basic MCP tools
4. Error handling and validation
5. Testing and documentation
6. Bonus features (caching, advanced queries)

Part 2: Agentic Workflow with Model Context Protocol
Extend the API you’ve built to support an agent-based interaction, where an AI agent can
interact with your API to fetch project data and execute actions based on context.
Requirements:
- Build a basic MCP-compatible tool or function that can interact with an agentic workflow
to complete the following:
- Accepts a prompt or plan (e.g., “Show me all overdue tasks assigned to Bob”)
- Calls your Project Tracker API internally
- Returns structured results usable by an LLM (e.g., JSON)
- Describe your internal prompt engineering technique and design
- Include at least one example of a user prompt, and simulate the full flow:
- Prompt → MCP → API → Structured Result → Agent response

MCP-Compatible Tool Implementation
Create an MCP server that includes these specific capabilities:
- Accepts natural language prompts (e.g., "Show me all overdue tasks assigned to Bob")
- Calls Project Tracker API internally using HTTP requests
- Returns structured JSON results optimized for LLM consumption
- Handles complex queries with intelligent parsing

Tools
- Projects and tasks

Prompt Engineering Design
Implement intelligent prompt parsing with these features:
- Intent Recognition: Classify user intent (status check, workload analysis, risk assessment)
- Entity Extraction: Extract project names, person names, dates, status filters
- Context Awareness: Handle follow-up questions and maintain conversation context
- Error Graceful Handling: Provide helpful suggestions when queries are ambiguous

Expected Flow:
1. Prompt → MCP Tool: Parse intent (overdue tasks + project impact)
2. MCP → API Calls:
    GET /api/tasks?assignedTo=Bob&status=overdue
    GET /api/projects (for affected projects)
3. API → Structured Result: JSON with tasks, projects, and impact analysis
4. Agent Response: Natural language summary with actionable insights

Requirement
- Include comprehensive error handling
- Add logging for debugging
- Support for structured responses

5. Technical Implementation Requirements
MCP Server Structure:
typescript// File: src/mcp/agenticServer.ts
- Use @modelcontextprotocol/sdk
- Implement StdioServerTransport
- Include comprehensive error handling
- Add logging for debugging
- Support for structured responses
Prompt Engineering Module:
typescript// File: src/mcp/promptEngine.ts
- Intent classification logic
- Entity extraction (NER-like functionality)
- Query parameter mapping
- Response formatting for LLMs
API Integration Layer:
typescript// File: src/mcp/apiClient.ts
- HTTP client for internal API calls
- Response caching and optimization
- Error handling and retries
- Data transformation utilities
6. Response Format Specification
Structure all MCP tool responses like this:
json{
  "summary": "Human-readable summary",
  "data": {
    "queryType": "overdue_tasks",  
    "results": [...],
    "metadata": {
      "totalCount": 5,
      "queryTime": "2025-01-28T10:30:00Z",
      "filters": {...}
    }
  },
  "insights": [
    "Bob has 3 overdue tasks affecting 2 critical projects",
    "Recommend immediate attention to Project Alpha deliverables"
  ],
  "recommendations": [
    "Schedule meeting with Bob to discuss priorities",
    "Consider reassigning Task #123 to reduce Bob's load"
  ]
}
7. Testing and Validation Requirements
Create test scenarios for:

Simple queries: "What's Bob's workload?"
Complex queries: "Show overdue tasks for marketing team in last 2 weeks"
Multi-step queries: "Project status + risk analysis + recommendations"
Edge cases: Invalid names, empty results, API errors

Include MCP Inspector setup:

Add npm script: "mcp:debug": "npx @modelcontextprotocol/inspector node dist/mcp/agenticServer.js"
Test all tools with sample data
Validate JSON structure and LLM compatibility

8. Documentation Requirements
Create comprehensive documentation covering:

Prompt Engineering Approach: How you handle natural language → API calls
Tool Descriptions: Each MCP tool's capabilities and examples
Flow Diagrams: Visual representation of prompt → response flow
Example Interactions: At least 5 different user scenarios with complete flows

9. Integration with Existing API
Ensure seamless integration:

No API Changes: MCP server calls existing REST endpoints
Authentication: Handle any API keys or tokens needed
Performance: Implement caching to avoid redundant API calls
Error Handling: Graceful degradation when API is unavailable

10. Code Quality Standards
Follow these standards:

TypeScript: Strict types, proper interfaces
Error Handling: Comprehensive try-catch with meaningful errors
Logging: Structured logging for debugging and monitoring
Testing: Unit tests for prompt parsing and API integration
Documentation: JSDoc comments for all public methods

Expected Deliverables
After implementation, I should have:

✅ Working MCP server with 4 intelligent tools
✅ Prompt engineering system that handles natural language
✅ Complete example flow with real data
✅ Comprehensive documentation of the approach
✅ Test scenarios demonstrating different query types
✅ Integration with existing Project Tracker API

Success Criteria
The implementation is successful when:

Natural Language Works: "Show Bob's overdue tasks" → correct API calls
Structured Responses: JSON output optimized for LLM consumption
Error Handling: Graceful handling of edge cases and API failures
Performance: Fast response times with intelligent caching
Documentation: Clear explanation of prompt engineering approach

Assessment Context
This demonstrates:

AI Integration Expertise: Modern agentic workflow understanding
System Design: Clean integration between MCP and REST API
Prompt Engineering: Sophisticated natural language processing
Enterprise Quality: Production-ready error handling and performance

When suggesting code, prioritize clarity and maintainability over cleverness.